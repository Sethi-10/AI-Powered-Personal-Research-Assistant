{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1145bd16-2552-4258-b380-e80010bc81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0646e49-8a6a-4e19-8bf3-8aa49ef871d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize HuggingFace Hub API token (you need to set this up)\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"***********\"\n",
    "\n",
    "# initialize Hub LLM\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 512, 'max_new_tokens' : 250, 'top_k' : 10, 'top_p': 0.95, 'repetition_penalty':1.03}\n",
    ")\n",
    "class ResearchAssistant:\n",
    "    def __init__(self):\n",
    "        self.cache_dir = Path(\"cache\")\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    def fetch_articles(self, query, max_results=5):\n",
    "        try:\n",
    "            search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=max_results,\n",
    "                sort_by=arxiv.SortCriterion.Relevance\n",
    "            )\n",
    "            return list(search.results())\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error fetching articles: {str(e)}\")\n",
    "\n",
    "    def summarize_text(self, text, max_length=50):\n",
    "        try:\n",
    "            summary = self.summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n",
    "            return summary[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error summarizing text: {str(e)}\")\n",
    "\n",
    "    def extract_keywords(self, text, num_keywords=5):\n",
    "        try:\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([text])\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "            sorted_indexes = np.argsort(tfidf_scores)[::-1]\n",
    "            return [feature_names[i] for i in sorted_indexes[:num_keywords]]\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error extracting keywords: {str(e)}\")\n",
    "\n",
    "    def named_entity_recognition(self, text):\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in named entity recognition: {str(e)}\")\n",
    "\n",
    "    def topic_modeling(self, texts, num_topics=3):\n",
    "        try:\n",
    "            processed_texts = [[token.lemma_ for token in nlp(text) if not token.is_stop and token.is_alpha] for text in texts]\n",
    "            dictionary = corpora.Dictionary(processed_texts)\n",
    "            corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "            lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "            return lda_model.print_topics()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in topic modeling: {str(e)}\")\n",
    "\n",
    "    def generate_insights(self, summaries):\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"summaries\"],\n",
    "            template=\"Based on the following article summaries, provide key insights and potential research directions:\\n\\n{summaries}\\n\\nInsights:\"\n",
    "        )\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        return chain.run(summaries=\"\\n\\n\".join(summaries))\n",
    "\n",
    "    def cache_key(self, query):\n",
    "        return hashlib.md5(query.encode()).hexdigest()\n",
    "\n",
    "    def get_cached_result(self, query):\n",
    "        key = self.cache_key(query)\n",
    "        cache_file = self.cache_dir / f\"{key}.json\"\n",
    "        if cache_file.exists():\n",
    "            with cache_file.open(\"r\") as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "\n",
    "    def cache_result(self, query, result):\n",
    "        key = self.cache_key(query)\n",
    "        cache_file = self.cache_dir / f\"{key}.json\"\n",
    "        with cache_file.open(\"w\") as f:\n",
    "            json.dump(result, f)\n",
    "\n",
    "    def research(self, query):\n",
    "        cached_result = self.get_cached_result(query)\n",
    "        if cached_result:\n",
    "            return cached_result\n",
    "\n",
    "        articles = self.fetch_articles(query)\n",
    "        result = []\n",
    "        summaries = []\n",
    "\n",
    "        for article in articles:\n",
    "            summary = self.summarize_text(article.summary)\n",
    "            keywords = self.extract_keywords(article.summary)\n",
    "            entities = self.named_entity_recognition(article.summary)\n",
    "            \n",
    "            article_data = {\n",
    "                \"title\": article.title,\n",
    "                \"authors\": [author.name for author in article.authors],\n",
    "                \"published\": article.published.isoformat(),\n",
    "                \"summary\": summary,\n",
    "                \"keywords\": keywords,\n",
    "                \"entities\": entities,\n",
    "                \"url\": article.pdf_url\n",
    "            }\n",
    "            result.append(article_data)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        topics = self.topic_modeling([article.summary for article in articles])\n",
    "        insights = self.generate_insights(summaries)\n",
    "\n",
    "        final_result = {\n",
    "            \"articles\": result,\n",
    "            \"topics\": topics,\n",
    "            \"insights\": insights\n",
    "        }\n",
    "\n",
    "        self.cache_result(query, final_result)\n",
    "        return final_result\n",
    "\n",
    "class ResearchAssistantGUI:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"AI Research Assistant\")\n",
    "        self.master.geometry(\"800x600\")\n",
    "        self.assistant = ResearchAssistant()\n",
    "        self.create_widgets()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.query_label = tk.Label(self.master, text=\"Enter your research topic:\")\n",
    "        self.query_label.pack(pady=10)\n",
    "\n",
    "        self.query_entry = tk.Entry(self.master, width=50)\n",
    "        self.query_entry.pack(pady=10)\n",
    "\n",
    "        self.search_button = tk.Button(self.master, text=\"Search\", command=self.perform_search)\n",
    "        self.search_button.pack(pady=10)\n",
    "\n",
    "        self.result_text = scrolledtext.ScrolledText(self.master, wrap=tk.WORD, width=80, height=30)\n",
    "        self.result_text.pack(pady=10, padx=10, expand=True, fill=tk.BOTH)\n",
    "\n",
    "    def perform_search(self):\n",
    "        query = self.query_entry.get()\n",
    "        if not query:\n",
    "            messagebox.showerror(\"Error\", \"Please enter a research topic\")\n",
    "            return\n",
    "\n",
    "        self.result_text.delete(1.0, tk.END)\n",
    "        self.result_text.insert(tk.END, \"Searching... Please wait.\\n\\n\")\n",
    "        self.master.update()\n",
    "\n",
    "        try:\n",
    "            result = self.assistant.research(query)\n",
    "            self.display_results(result)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "    def display_results(self, result):\n",
    "        self.result_text.delete(1.0, tk.END)\n",
    "\n",
    "        for i, article in enumerate(result[\"articles\"], 1):\n",
    "            self.result_text.insert(tk.END, f\"Article {i}:\\n\")\n",
    "            self.result_text.insert(tk.END, f\"Title: {article['title']}\\n\")\n",
    "            self.result_text.insert(tk.END, f\"Authors: {', '.join(article['authors'])}\\n\")\n",
    "            self.result_text.insert(tk.END, f\"Published: {article['published']}\\n\")\n",
    "            self.result_text.insert(tk.END, f\"Summary: {article['summary']}\\n\")\n",
    "            self.result_text.insert(tk.END, f\"Keywords: {', '.join(article['keywords'])}\\n\")\n",
    "            self.result_text.insert(tk.END, f\"Named Entities: {', '.join([f'{ent[0]} ({ent[1]})' for ent in article['entities']])}\\n\")\n",
    "            self.result_text.insert(tk.END, f\"URL: {article['url']}\\n\")\n",
    "            self.result_text.insert(tk.END, \"-\" * 50 + \"\\n\")\n",
    "\n",
    "        self.result_text.insert(tk.END, \"\\nTopics:\\n\")\n",
    "        for topic in result[\"topics\"]:\n",
    "            self.result_text.insert(tk.END, f\"{topic}\\n\")\n",
    "\n",
    "        self.result_text.insert(tk.END, \"\\nInsights:\\n\")\n",
    "        self.result_text.insert(tk.END, result[\"insights\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ResearchAssistantGUI(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8292f-da78-4928-a962-bc93f2de004a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
